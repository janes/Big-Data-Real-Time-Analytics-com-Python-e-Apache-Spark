{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sl.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Pima Indians Diabetes Data Set https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset descreve os registros médicos entre pacientes do Pima Inidians e cada registro está marcado se o paciente desenvolveu ou não diabetes.\n",
    "\n",
    "### Informações sobre os atributos:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo e Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o arquivo csv com o NumPy \n",
    "import numpy as np\n",
    "arquivo = 'data/pima-data.csv'\n",
    "arquivo_data = open(arquivo, 'rb')\n",
    "dados = np.loadtxt(arquivo_data, delimiter=',')\n",
    "dados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando arquivo csv usando pandas\n",
    "import pandas as pd\n",
    "arquivo = 'data/pima-data.csv'\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = pd.read_csv(arquivo, names=colunas)\n",
    "dados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Método recomendado\n",
    "# Carregando arquivo csv usando pandas e através da internet\n",
    "import pandas as pd\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.cs\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dados = pd.read_csv(arquivo, names=colunas)\n",
    "dados.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estatística Descritiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando as primeiras 20 linhas\n",
    "dados.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o número de linhas no seu arquivo for muito grande, o algoritmo pode levar muito tempo para ser treinado. Se o número de registros for muito pequeno, você pode não ter registros suficientes para treinar seu modelo.\n",
    "\n",
    "Se você tiver muitas colunas em seu arquivo, o algoritmo pode apresentar problemas de performance devido a alta dimensionalidade.\n",
    "\n",
    "A melhor solução vai depender de cada caso. Mas lembre-se: treine seu modelo em um subset do seu conjunto maior de dados e depois aplique o modelo no Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando as dimensões\n",
    "dados.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tipo dos dados é muito importante. Pode ser necessário converter strings ou colunas com números inteiros podem representar variáveis categóricas ou valores ordinários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo de dados de cada atributo\n",
    "dados.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumário estatístico\n",
    "dados.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em problemas de classificação pode ser necessário balancear as classes. Classes desbalanceadas (ou seja, volume maior de um dos tipos das classes) são comuns e precisam ser tratadas durante a fase de pré-processamento. Podemos ver abaixo que existe uma clara desproporção entre as classes 0 (não ocorrência de diabetes) e 1 (ocorrência de diabetes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conferindo a distribuição das classes\n",
    "dados.groupby('class').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlação é o relacionamento entre 2 variáveis. O métodos mais comum para calcular correlação é o método de Pearson, que assume uma distribuição normal dos dados. Correlação de -1 mostra uma correlação negativa, enquanto uma correlação de +1 mostra uma correlação positiva. Uma correlação igual a 0 mostra que não há relacionamento entre as variáveis.\n",
    "\n",
    "Alguns algoritmos como regressão linear e regressão logística podem apresentar problemas de performance se houverem atributos altamente correlacionados (colineares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlação de Pearson\n",
    "dados.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skew (ou simetria) se refere a distribuição dos dados que é assumida ser normal ou gaussiana (bell curve). Muitos algoritmos de Machine Learning consideram que os dados possuem uma distribuição normal. Conhecendo a simetria dos dados, permite que você faça uma preparação e entregue o que o algoritmo espera receber, aumentado desta forma a acurácia do modelo preditivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o shewde cada atributo\n",
    "dados.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando com Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o histograma podemos rapidamente avaliar a distribuição de cada atributo. Os histograma agrupam os dados em bins e fornecem uma contagem do número de observações em cada bin. Com o histograma, você pode rapidamente verificar a simetria dos dados e se eles estão em distribuição normal ou não. Isso também vai ajudar na identificação dos outliers.\n",
    "\n",
    "Podemos ver que os atributos age, pedi e test possuem uma distribuição exponencial. Podemos ver que as colunas mass e press possuem uma distribuição normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma Univalorado\n",
    "dados.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os Density Plots são outra forma de visualizar a distribuição dos dados para cada atributo. O plot é como uma espécie de histograma abstrato com uma curva suave através do topo dos bins de um histograma. Pode ser mais fácil identificar a distribuição dos dados usando um density plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot Univariado\n",
    "dados.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os boxplots também podemos revisar a distribuição dos dados para cada atributo. A linha no centro (vermelho) é o valor da mediana (quartil 50%), a linha abaixo é o quartil 25% e a linha acima o quartil 75%. O boxplot ajuda a ter uma ideia da dispersão dos dados e os possíveis outliers.\n",
    "\n",
    "Podemos ver que a dispersão dos dados é bem diferente entre os atributos. As colunas age, skin e test possuem uma simetria muito próxima a valores de dados menores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and whisker plots\n",
    "dados.plot(kind = 'box', subplots = True, layout = (3,3), sharex = False, sharey = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlação com nomes das variáveis\n",
    "\n",
    "correlations = dados.corr()\n",
    "\n",
    "# plot\n",
    "import numpy as np\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin = -1, vmax = 1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,9,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(colunas)\n",
    "ax.set_yticklabels(colunas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlação genérica\n",
    "\n",
    "correlations = dados.corr()\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin = -1, vmax = 1)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um scatterplot mostra o relacionamento entre duas variáveis como pontos em duas dimensões, sendo um eixo para cada variável. Podemos criar um scatterplot para cada par de variáveis em nosso dataset. A exemplos da matriz de correlação, o scatterplot matrix é simétrico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(dados)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando com Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot com orientação vertical\n",
    "sns.boxplot(data=dados, orient='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustermap\n",
    "sns.clustermap(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kdeplot\n",
    "sns.kdeplot(dados.age, dados.mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "sns.distplot(dados.pedi, fit = stats.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os Dados para Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitos algoritmos esperam receber os dados em um formato específico. É seu trabalho preparar os dados em uma estrutura que seja adequada ao algoritmo que você está utilizando.\n",
    "\n",
    "É muito provável que você tenha que realizar tarefas de pré-processamento nos dados. Esse é um passo necessário dentro do processo. O desafio é o fato que cada algoritmo requer uma estrutura diferente, o que pode requerer transformações diferentes nos dados. Mas é possível em alguns casos, obter bons resultados sem um trabalho de pré-processamento. Mas é uma boa prática criar diferentes visões e transformações dos dados, de modo a poder testar diferentes algoritmos de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E uma das primeiras tarefas dentro do pré-processamento, é colocar seus dados na mesma escala. Muitos algoritmos de Machine Learning vão se beneficiar disso e produzir resultados melhores. Esta etapa também é chamada de normalização e significa colocar os dados em uma escala com range entre 0 e 1. Isso é útil para a otimização, sendo usado no core dos algoritmos de Machine Learning, como gradient descent. Isso também é útil para algoritmos como regressão e redes neurais e algoritmos que usam medidas de distância, como KNN. O scikit-learn possui uma função para esta etapa, chamada MinMaxScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando os dados para a mesma escala (entre 0 e 1)\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Gerando a nova escala\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledX = scaler.fit_transform(x)\n",
    "\n",
    "# Sumarizando os dados transformados\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padronização é a técnica para transformar os atributos com distribuição Gaussiana (normal) e diferentes médias e desvios padrões em uma distribuição Gaussiana com a média igual a 0 e desvio padrão igual a 1. Isso é útil para algoritmos que esperam que os dados estejam com uma distribuição Gaussiana, como regressão linear, regressão logística e linear discriminant analysis. Funciona bem quando os dados já estão na mesma escala. O scikit-learn possui uma função para esta etapa, chamada StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizando os dados (0 para a média, 1 para desvio padrão)\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Gerando o novo padrão\n",
    "scaler = StandardScaler().fit(x)\n",
    "standardX = scaler.transform(x)\n",
    "\n",
    "# Sumarizando os dados transformados\n",
    "print(standardX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No scikit-learn, normalização se refere a ajustar a escala de cada observação (linha) de modo que ela tenha comprimento igual a 1 (chamado vetor de comprimento 1 em álgebra linear). Este método de pré-processamento é útil quando temos datasets esparsos (com muitos zeros) e atributos com escala muito variada. Útil quando usamos algoritmos de redes neurais ou que usam medida de distância, como KNN. O scikit-learn possui uma função para esta etapa, chamada Normalizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando os dados (comprimento igual a 1)\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Gerando os dados normais\n",
    "scaler = Normalizer().fit(x)\n",
    "nomalizedX = scaler.transform(x)\n",
    "\n",
    "# Sumarizando os dados transformados\n",
    "print(nomalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando os Dados em Valores Binários (Binarizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós podemos definir um valor em nossos dados, ao qual chamamos de threshold e então definimos que todos os valores acima do threshold serão marcados como sendo 1 e todos valores iguais ou abaixo do threshold serão marcados como sendo 0. Isso é o que chamamos de Binarizing. Isso é útil quando temos probabilidades e queremos transformar os dados em algo com mais significado. O scikit-learn possui uma função para esta etapa, chamada Binarizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarização\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Gerando a binarização\n",
    "binarizer = Binarizer(threshold=0.0).fit(x)\n",
    "binaryX = binarizer.transform(x)\n",
    "\n",
    "# Sumarizando os dados transformados\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os atributos presentes no seu dataset e que você utiliza nos dados de treino, terão grande influência na precisão e resultado do seu modelo preditivo. Atributos irrelevante terão impacto negativo na performance, enquanto atributos colineares podem afetar o grau de acurácia do modelo. O Scikit-learn possui funções que automatizam o trabalhao de extração e seleção de variáveis.\n",
    "\n",
    "A etapa de Feature Selection é onde selecionamos os atributos (variáveis) que serão melhores candidatas a variáveis preditoras. O Feature Selection nos ajuda a reduzir o overfitting (quando o algoritmo aprende demais), aumenta a acurácia do modelo e reduz o tempo de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção Univariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testes estatísticos podem ser usados para selecionar os atributos que possuem forte relacionamento com a variável que estamos tentando prever. O Scikit-learn fornece a função SelectKBest() que pode ser usada com diversos testes estatísticos, para selecionar os atributos. Vamos usar um teste que estudamos no capítulo anterior, o teste qui-quadrado. Vamos seelcionar os 4 melhores atributos que podem ser usados como variáveis preditoras.\n",
    "\n",
    "Veremos abaixo o score para cada atributo e os 4 atributos com maior score e que portanto devem ser selecionados como variáveis preditoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração de variáveis com Testes Estatísticos Univalorados (Teste qui-quadrado)\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Extração de variaveis\n",
    "test = SelectKBest(score_func=chi2, k = 4)\n",
    "fit = test.fit(x, y)\n",
    "\n",
    "# Sumarizando o score\n",
    "print(fit.scores_)\n",
    "features = fit.transform(x)\n",
    "\n",
    "# Sumarizando atributos selecionados\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminação Recursiva de Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é outra técnica para seleção de atributos, que recursivamente remove os atributos e constrói o modelo com os atributos remanescentes. Esta técnica utiliza a acurácia do modelo para identificar os atributos que mais contribuem para prever a variável alvo. Em inglês esta técnia é chamada Recursive Feature Elimination (RFE).\n",
    "\n",
    "O exemplo abaixo utiliza a técnica de eliminação recursiva de atributos com um algoritmo de Regressão Logística para selecionar as 3 melhores variáveis preditoras. O RFE selecionou as variáveis preg, mass e pedi, que estão marcadas como True em \"Atributos Selecionados\" e com valor 1 em \"Ranking dos Atributos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminação Recursiva de variaveis\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Criação do modelo\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# RFE\n",
    "rfe = RFE(modelo, 3)\n",
    "fit = rfe.fit(x, y)\n",
    "\n",
    "# Print dos resultados\n",
    "print(\"Número de Atributos: %d\" % fit.n_features_)\n",
    "print(df.columns[0:8])\n",
    "print(\"Atributos Selecionados: %s\" % fit.support_)\n",
    "print(\"Ranking dos Atributos: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Ensemble para Seleção de Variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagged Decision Trees, como o algoritmo RandomForest, podem ser usados para estimar a importância de cada atributo. Esse método retorna um score para cada atributo.\n",
    "\n",
    "Quanto maior o score, maior a importância do atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancia do atributo com o Extra Trees Classifier\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Criação do modelo - feature selection\n",
    "modelo = ExtraTreesClassifier()\n",
    "modelo.fit(x, y)\n",
    "\n",
    "# Print dos resultados\n",
    "print(df.columns[0:8])\n",
    "print(modelo.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O PCA foi inventado em 1901 por Karl Pearson e utiliza álgebra linear para transformar datasets em uma forma comprimida, o que é geralmente conhecido como Redução de Dimensionalidade. Com PCA você pode escolher o número de dimensões (chamados componentes principais) no resultado transformado. Vamos usar PCA para selecionar 3 componentes principais.\n",
    "\n",
    "A Análise de Componentes Principais (PCA) é um método para extração das variáveis importantes (na forma de componentes) a partir de um grande conjunto de variáveis, disponíveis em um conjunto de dados. Esta técnica permite extrair um número pequenos de conjuntos dimensionais a partir de um dataset altamente dimensional. Com menos variáveis a visualização também se torna muito mais significativa. PCA é mais útil quando se lida com 3 ou mais dimensões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image source: http://www.nlpca.org/pca_principal_component_analysis.html\n",
    "from IPython.display import Image\n",
    "Image(url = 'images/PCA2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada componente resultante é uma combinação linear de n atributos. Cada componente principal é uma combinação de atributos presentes no dataset. O Primeiro Componente Principal é a combinação linear dos atributos com máxima variância e determina a direção em que há mais alta variabilidade nos dados. Quanto maior a variabilidade capturada no primeiro componente principal, mais informação será capturada pelo componente. O Segundo Componente Principal captura a variabilidade remanescente. Todos os componentes subsequentes possuem o mesmo conceito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image source: http://www.nlpca.org/pca_principal_component_analysis.html\n",
    "from IPython.display import Image\n",
    "Image(url = 'images/PCA3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O PCA precisa ser alimentado com dados normalizados. Utilizar o PCA em dados não normalizados pode gerar resultados inesperados.\n",
    "\n",
    "A análise de componentes principais é uma técnica da estatística multivariada que consiste em transformar um conjunto de variáveis originais em outro conjunto de variáveis denominadas de componentes principais. Os componentes principais apresentam propriedades importantes: cada componente principal é uma combinação linear de todas as variáveis originais, são independentes entre si e estimados com o propósito de reter, em ordem de estimação, o máximo de informação, em termos da variação total contida nos dados. Os componentes principais são garantidamente independentes apenas se os dados forem normalmente distribuídos (conjuntamente).\n",
    "\n",
    "Procura-se redistribuir a variação observada nos eixos originais de forma a se obter um conjunto de eixos ortogonais não correlacionados. Esta técnica pode ser utilizada para geração de índices e agrupamento de indivíduos. A análise agrupa os indivíduos de acordo com sua variação, isto é, os indivíduos são agrupados segundo suas variâncias, ou seja, segundo seu comportamento dentro da população, representado pela variação do conjunto de características que define o indivíduo, ou seja, a técnica agrupa os indivíduos de uma população segundo a variação de suas características.\n",
    "\n",
    "A análise de componentes principais é associada à idéia de redução de massa de dados, com menor perda possível da informação.\n",
    "\n",
    "O objetivo é sumarizar os dados que contém muitas variáveis (p) por um conjunto menor de variáveis (k) compostas derivadas a partir do conjunto original. PCA usa uma conjunto de dados representado por uma matriz de n registros por p atributos, que podem estar correlacionados, e sumariza esse conjunto por eixos não correlacionados (componentes principais) que são uma combinação linear das p variáveis originais. As primeiras k componentes contém a maior quantidade de variação dos dados.\n",
    "\n",
    "Em termos gerais a PCA busca reduzir o número de dimensões de um dataset, projetando os dados em um novo plano. Usando essa nova projeção os dados originais, que podem envolver diversas variáveis, podem ser interpretados utilizando menos \"dimensões.\"\n",
    "\n",
    "No dataset reduzido podemos observar com mais clareza tendências, padrões e/ou outliers. Mas vale lembrar que a regra: \"Se não está nos dados brutos não existe!\" é sempre válida. A PCA fornece apenas mais clareza aos padrões que já estão lá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = 'images/PCA.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto maior a variância, maior a quantidade de informação contida no componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Seleção de atributos\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(x)\n",
    "\n",
    "# Sumarizando os componentes\n",
    "print(\"Variância: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você precisa saber se seu modelo preditivo vai funcionar bem quando receber novos dados. A melhor maneira de avaliar a performance do modelo é fazer previsões em dados que você já conhece o resultado. Outra maneira de testar a performance do seu modelo é utilizar técnicas estatísticas como métodos de amostragem que permitem você estimar quão bem seu modelo irá fazer previsões em novos dados.\n",
    "\n",
    "A avaliação do modelo é uma estimativa de quão bem o algoritmo será capaz de prever em novos dados. Isso não garante performance. Após avaliar o modelo, nós podemos treiná-lo novamente com os dados de treino e então prepará-lo para uso operacional em produção. Existem diversas técnicas para isso e estudaremos duas aqui: Conjunto de dados de treino e de teste e Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de Treino e de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é o método mais utilizado para avaliar performance de um algoritmo de Machine Learning. Dividimos nossos dados originais em dados de treino e de teste. Treinamos o algoritmo nos dados de treino e fazemos as previsões nos dados de teste e avaliamos o resultado. A divisão dos dados vai depender do seu dataset, mas utiliza-se com frequência tamanhos entre 70/30 (treino/teste) e 65/35 (treino/teste).\n",
    "\n",
    "Este método é bem veloz e ideal para conjuntos de dados muito grandes. O ponto negativo é a possibilidade de alta variância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação usando dados de treino e de teste\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho das amostras\n",
    "teste_size = 0.33\n",
    "\n",
    "# Garante que os resultados podem ser reproduzidos\n",
    "# isso é importante para comparar a acurácia com outros algoritmos de Machine Learning \n",
    "seed = 7\n",
    "\n",
    "# Criado o conjunto de dados de treino e de teste\n",
    "x_treino, x_teste, y_treino, y_teste = model_selection.train_test_split(x, y, \n",
    "                                                                        test_size = teste_size,\n",
    "                                                                        random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(x_treino, y_treino)\n",
    "\n",
    "# Score\n",
    "result = modelo.score(x_teste, y_teste)\n",
    "print(\"Acurácia: %.3f%%\" % (result * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation é uma técnica que pode ser utilizada para avaliar a performance de um modelo com menos variância que a técnica de dividir os dados em treino/teste. Com esta técnica dividimos os dados em partes normalmente chamadas de k-folds (por exemplo k = 5, k = 10). Cada parte é chamada fold. O algoritmo é treinado em k-1 folds. Cada fold é usado no treinamento de forma repetida e um fold por vez. Após executar o processo em k-1 folds, podemos sumarizar a performance em cada fold usando a média e o desvio padrão (Eu disse que Estatística era importante no processo de Big Data Analytics). O resultado é normalmente mais confiável e oferece maior acurácia ao modelo. A chave deste processo está em definir o correto valor de k, de modo que o número de folds represente adequadamente o número de repetições necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = 'images/cross-validation.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação usando Cross Validation\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo os valores para os folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Usando a média e o desvio padrão\n",
    "print(\"Acurácia: %.3f%% (%.3f%%)\" % (resultado.mean()*100.0, resultado.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando a Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As métricas que você escolhe para avaliar a performance do modelo vão influenciar a forma como a performance é medida e comparada com modelos criados com outros algoritmos.\n",
    "\n",
    "Vamos utilizar o mesmo algoritmo, mas com métricas diferentes e assim comparar os resultados. A função cross_validation.cross_val_score() será usada para avaliar a performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas para Algoritmos de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia\n",
    "# Número de previsões corretas.\n",
    "# É util apenas quando existe o mesmo número de observações em cada classe\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo os valores para os folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring = 'accuracy')\n",
    "\n",
    "# Usando a média e o desvio padrão\n",
    "print(\"Acurácia: %.3f (%.3f)\" % (resultado.mean(), resultado.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC \n",
    "# A Curva ROC permite analisar a métrica AUC (Area Under the Curve).\n",
    "# Essa é uma métrica de performance para classificação binária, em que podemos definir as classes em positiavs e negativas.\n",
    "# Problemas de classificação binária são um trade-off sentre Sensitivity e Specifity.\n",
    "# Sensitivity é a taxa de verdadeiros positivos (TP). Ese é o número de instâncias positivas da primeira classe que foram previstas corretamente.\n",
    "# Specifity é a taxa de verdadeiros negativos (TN). Esse é o número de instâncias da segunda classe que foram previstas corretamente.\n",
    "# Valores acima de 0.5 indicam uma boa taxa de previsão.\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo os valores para os folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring = 'roc_auc')\n",
    "\n",
    "# Usando a média e o desvio padrão\n",
    "print(\"Acurácia: %.3f (%.3f)\" % (resultado.mean(), resultado.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "# Permite verificar a acurácia de um modelo com duas classes\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho do conjunto de dados\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "# Criado o conjunto de dados de treino e de teste\n",
    "x_treino, x_teste, y_treino, y_teste = model_selection.train_test_split(x, y, \n",
    "                                                                        test_size = teste_size,\n",
    "                                                                        random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(x_treino, y_treino)\n",
    "\n",
    "# Fazendo as previsões e construindo a Confusion Matrix\n",
    "previsoes = modelo.predict(x_teste)\n",
    "matrix = confusion_matrix(y_teste, previsoes)\n",
    "\n",
    "# Imprimindo a Confusion Matrix\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logarithmic Loss\n",
    "# Avalia as previsões de probabilidade de um membro pertencer a uma determinada classe. \n",
    "# Valores menores indicam boa performance, com 0 representando um perfeito logloss. A função cross_val_score() inverte o valor.\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring='neg_log_loss')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(\"Logloss : %.3f (%.3f)\" %(resultado.mean(), resultado.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatório de classificação\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho do conjunto de dados\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "# Criado o conjunto de dados de treino e de teste\n",
    "x_treino, x_teste, y_treino, y_teste = model_selection.train_test_split(x, y, \n",
    "                                                                        test_size = teste_size,\n",
    "                                                                        random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(x_treino, y_treino)\n",
    "\n",
    "# Fazendo as previsões e construindo a Confusion Matrix\n",
    "previsoes = modelo.predict(x_teste)\n",
    "report = classification_report(y_teste, previsoes)\n",
    "\n",
    "# Imprimindo a Confusion Matrix\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas para Algoritmos de Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos agora estudar as métricas para regressão, usaremos outro dataset, o Boston Houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "# Mean Absolute Error\n",
    "# É a soma da diferença absoluta entre previsões e valores reais.\n",
    "# Fornece uma ideia de quão erradas estão nossas previsões.\n",
    "# Valor igual a 0 indica que não há erro, sendo a previsão perfeita (a exemplo do Logloss, a função cross_val_score inverte o valor) \n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LinearRegression()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(\"MAE : %.3f (%.3f)\" %(resultado.mean(), resultado.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "# Mean Square Error\n",
    "# Similar ao MAE, fornece a magnitude do erro do modelo.\n",
    "# Ao extrairmos a raiz quadrada do MSE convertemos as unidades de volta ao original, o que pode ser útil para descrição e apresentação.\n",
    "# Isso é chamado RMSE (Root Mean Squared Error)\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LinearRegression()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(\"MAE : %.3f (%.3f)\" %(resultado.mean(), resultado.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2\n",
    "# Essa métrica fornece uma indicação do nível de precisão das previsões em relação aos valores observados.\n",
    "# Também chamado de coeficiente de determinação.\n",
    "# Valores entre 0 e 1, sendo 0 o valor ideal.\n",
    "\n",
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LinearRegression()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring='r2')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(\"MAE : %.3f (%.3f)\" %(resultado.mean(), resultado.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não temos como saber qual algoritmo vai funcionar melhor na construção do modelo, antes de testarmos o algoritmo com nosso dataset. O ideal é testar alguns algoritmos e então escolher o que fornece melhor nível de precisão. Vamos testar um conjunto de algoritmos de classificação, nas mesmas condições.\n",
    "\n",
    "Utilizaremos o dataset Pima Indian Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Linear. O algoritmo de Regressão Logística assume que seus dados estão em uma Distribuição Normal para valores numéricos que podem ser modelados com classificação binária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado)\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Linear. Técnica estatística para classificação binária. Também assume que os dados estão em Distribuição Normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LinearDiscriminantAnalysis()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN - K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Não-Linear que utiliza uma métrica de distância para encontrar o valor de K mais adequado as instâncias do dataset de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "random_state = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = random_state)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = KNeighborsClassifier()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Não-Linear. Calcula a Probabilidade de cada classe e a probabilidade condicional de cada classe dado uma variável de entrada. As probabilidades são então estimadas para os novos dados e multiplicadas, assumindo que são independentes (suposição simples ou Naive). Assume dados em distirbuição Gaussiana (Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = GaussianNB()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART (Classification and Regression Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Não-Linear. O algoritmo CART constrói uma árvore binária a partir do dataset de treino. Cada atributo e cada valor de cada atributo são avaliados com o objetivo de reduzir a função de custo (Cost Function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = DecisionTreeClassifier()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo deste modelo é buscar uma linha que melhor separa duas classes dentro de um conjunto de dados. As instâncias de dados que estão mais próximas desta linha que separa as classes, são chamadas support vectors. O SVM tem sido estendido para suportar multiclasses.\n",
    "\n",
    "Support Vector Machines são algoritmos de classificação muito poderosos. Quando usados em conjunto com “Random forest” e outras ferramentas de aprendizagem automática, dão uma dimensão muito diferente para montagem de modelos. Assim, eles se tornam cruciais para os casos em que é necessária um poder de previsão muito elevado. Esses algoritmos são um pouco mais difíceis de visualizar devido à complexidade na formulação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = SVC()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume que os dados estão em Distribuição Normal e também assume que as variáveis são relevantes para a construção do modelo e que não sejam colineares, ou seja, variáveis com alta correlação (cabe a você, Cientista de Dados, entregar ao algoritmo as variáveis realmente relevantes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LinearRegression()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensão para a regressão linear onde a loss function é modificada para minimizar a complexidade do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = Ridge()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regression é uma modificação da regressão linear e assim como a Ridge Regression, a loss function é modificada para minimizar a complexidade do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = Lasso()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet é uma forma de regularização da regressão que combina as propriedades da regressão Ridge e LASSO. O objetivo é minimizar a complexidade do modelo, penalizando o modelo usando a soma dos quadrados dos coeficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = ElasticNet()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = KNeighborsRegressor()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(n_splits=num_folds, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = DecisionTreeRegressor()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/boston-houses.csv\"\n",
    "colunas = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']\n",
    "df = read_csv(url, delim_whitespace=True, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:13]\n",
    "y = array[:,13]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(n_splits=num_folds, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = SVR()\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção do Modelo Preditivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veremos que os algoritmos de Regressão Logística e Linear Discriminant Analysis apresentaram o melhor nível de precisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import  KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Preparando os modelos\n",
    "modelos = []\n",
    "modelos.append(('LR', LogisticRegression()))\n",
    "modelos.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "modelos.append(('NB', GaussianNB()))\n",
    "modelos.append(('KNN', KNeighborsClassifier()))\n",
    "modelos.append(('CART', DecisionTreeClassifier()))\n",
    "modelos.append(('SVM', SVC()))\n",
    "\n",
    "# Avaliando cada modelo\n",
    "resultados = []\n",
    "nomes = []\n",
    "\n",
    "for nome, modelo in modelos:\n",
    "    kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "    CV_results = model_selection.cross_val_score(modelo, x, y, cv = kfold, scoring='accuracy')\n",
    "    resultados.append(CV_results)\n",
    "    nomes.append(nome)\n",
    "    msg = \"%s: %f (%f)\" % (nome, CV_results.mean(), CV_results.std())\n",
    "    print(msg)\n",
    "\n",
    "    \n",
    "# Boxplot para comparar os algoritmos\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"Comparação de Algoritmos de Classificação\")\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(resultados)\n",
    "ax.set_xticklabels(nomes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimizando o modelo - Ajuste de parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os algoritmos de Machine Learning são parametrizados, o que significa que você pode ajustar a performance do seu modelo preditivo, através do tuning (ajuste fino) dos parâmetros. Seu trabalho é encontrar a melhor combinação entre os parâmetros em cada algoritmo de Machine Learning. Esse processo também é chamado de Otimização Hyperparâmetro. O scikit-learn oferece dois métodos para otimização automática dos parâmetros: Grid Search Parameter Tuning e Random Search Parameter Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método realiza metodicamente combinações entre todos os parâmetros do algoritmo, criando um grid. Vamos experimentar este método utilizando o algoritmo de Regressão Ridge. No exemplo abaixo veremos que o valor 1 para o parâmetro alpha atingiu a melhor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo os valores que serão testados\n",
    "valores_alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "valores_grid = dict(alpha = valores_alphas)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = Ridge()\n",
    "\n",
    "# Criando o grid\n",
    "grid = model_selection.GridSearchCV(estimator=modelo, param_grid=valores_grid)\n",
    "grid.fit(x, y)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método gera amostras dos parâmetros dos algoritmos a partir de uma distribuição randômica uniforme para um número fixo de interações. Um modelo é construído e testado para cada combinação de parâmetros. Neste exemplo veremos que o valor muito próximo de 1 para o parâmetro alpha é o que vai apresentar os melhores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo os valores que serão testados\n",
    "valores_grid = {'alpha': uniform()}\n",
    "seed = 7\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = Ridge()\n",
    "iterations = 100\n",
    "rsearch = model_selection.RandomizedSearchCV(estimator=modelo,\n",
    "                                            param_distributions = valores_grid,\n",
    "                                            n_iter = iterations,\n",
    "                                            random_state = seed)\n",
    "rsearch.fit(x, y)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(rsearch.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o Resultado do seu trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho dos dados de treino e de teste\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "# Criando o dataset de treino e de teste\n",
    "x_treino, x_teste, y_treino, y_teste = model_selection.train_test_split(x, y,\n",
    "                                                                       test_size = teste_size,\n",
    "                                                                       random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(x_treino, y_treino)\n",
    "\n",
    "# Salvando o modelo\n",
    "arquivo = 'model/modelo_v1.sav'\n",
    "pickle.dump(modelo, open(arquivo, 'wb'))\n",
    "\n",
    "# Carregando o arquivo\n",
    "modelo_v1 = pickle.load(open(arquivo, 'rb'))\n",
    "resultado = modelo_v1.score(x_teste, y_teste)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimizando Performance com Métodos Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos Ensemble permitem aumentar consideravelmente o nível de precisão nas suas previsões. Veremos como criar alguns dos Métodos Ensemble mais poderosos em Python. Existem 3 métodos principais para combinar previsões a partir de diferentes modelos:\n",
    "\n",
    "Bagging - Para construção de múltiplos modelos (normalmente do mesmo tipo) a partir de de diferentes subsets no dataset de treino.\n",
    "\n",
    "Boosting - Para construção de múltiplos modelos (normalmente do mesmo tipo), onde cada modelo aprende a corrigir os erros gerados pelo modelo anterior, dentro da sequência de modelos criados.\n",
    "\n",
    "Voting - Para construção de múltiplos modelos (normalmente do tipos diferentes) e estatísticas simples (como a média) são usadas para combinar as previsões.\n",
    "\n",
    "Vejamos como utilizar estes métodos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método funciona bem quando existe alta variância nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os ddos em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state = seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "\n",
    "# Definindo o numero de trees\n",
    "num_trees = 100\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = BaggingClassifier(base_estimator = cart, n_estimators = num_trees, random_state = seed)\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest é uma extensão do Baggig Decision Tree. Amostras do dataset de treino são usadas com reposição, mas as árvores são criadas de uma forma que reduz a correlação entre classificadores individuais (Random Forest é um conjunto de árvores de decisão)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Definindo o numero de trees\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = RandomForestClassifier( n_estimators = num_trees, max_features = max_features)\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmos baseados em Boosting Ensemble criam uma sequência de modelos que tentam corrigir os eros dos modelos anteriores dentro da sequência. Uma vez criados, os modelos fazem previsões que podem receber um peso de acordo com sua acurácia e os resultados são combinados para criar uma previsão única final.\n",
    "\n",
    "O AdaBoost atribui pesos às instâncias no dataset, definindo quão fácil ou difícil elas são para o processo de classificação, permitindo que o algoritmo tenha mais ou menos atenção às instâncias durante o processo de construção dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Definindo o numero de trees\n",
    "num_trees = 30\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = AdaBoostClassifier( n_estimators = num_trees, random_state = seed)\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também chamado Stochastic Gradient Boosting, é um dos métodos Ensemble mais sofisticados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Definindo o numero de trees\n",
    "num_trees = 100\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = GradientBoostingClassifier( n_estimators = num_trees, random_state = seed)\n",
    "resultado = model_selection.cross_val_score(modelo, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é um dos métodos Ensemble mais simples. Este método cria dois ou mais modelos separados a partir do dataset de treino. O Classificador Voting então utiliza a média das previsões de cada sub-modelo para fazer as previsões em novos conjuntos de dados. As previsões de cada sub-modelo podem receber pesos, através de parâmetros definidos manualmente ou através de heurística. Existem versões mais avançadas do Voting, em que o modelo pode aprender o melhor peso a ser atribuído aos sub-modelos. Isso é chamado Stacked Aggregation, mas ainda não está disponível no Scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o valores para o numero de folds\n",
    "num_folds = 10\n",
    "num_instances = len(x)\n",
    "seed = 7\n",
    "\n",
    "# Separando os dados em folds\n",
    "kfold = model_selection.KFold(num_folds, True, random_state=seed)\n",
    "\n",
    "# Criando os sub-modelos\n",
    "estimators = []\n",
    "\n",
    "modelo1 = LogisticRegression()\n",
    "estimators.append(('logistic', modelo1))\n",
    "\n",
    "modelo2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', modelo2))\n",
    "\n",
    "modelo3 = SVC()\n",
    "estimators.append(('svm', modelo3))\n",
    "\n",
    "# Criando o modelo ensemble\n",
    "ensemble = VotingClassifier( estimators)\n",
    "resultado = model_selection.cross_val_score(ensemble, x, y, cv = kfold)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(resultado.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo XGBoost - Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo XGBoost é uma extensão do GBM (Gradient Boosting Method) que permite trabalhar com multithreading em uma única máquina e processamento paralelo em um cluster de vários servidores. A principal vantagem do XGBoost sobre o GBM é sua capacidade de gerenciar dados esparsos. O XGBoost automaticamente aceita dados esparsos como input sem armazenar zeros na memória.\n",
    "\n",
    "Principais vantagensdo XGBoost:\n",
    "\n",
    "1- Aceita dados esparsos (o que permite trabalhar com matrizes esparsas), sem a necessidade de conversão para matrizes densas.\n",
    "\n",
    "2- Constrói uma árvore de aprendizagem utilizando um moderno método de split (chamado quatile sketch), o que resulta em tempo de processamento muito menor que métodos tradicionais.\n",
    "\n",
    "3- Permite computação paralela em uma única máquina (através do uso de multithreading) e processamento paralelo em máquinas distribuídas em cluster.\n",
    "\n",
    "Basicamente o XGBoost utiliza os mesmos parâmetros do GBM e permite tratamento avançado de dados missing.\n",
    "\n",
    "O XGBoost é muito utilizado por Cientistas de Dados que vencem competições no Kaggle. Repositório no Github: https://github.com/dmlc/XGBoost\n",
    "\n",
    "#### Instalar XGBoost a partir do PyPi\n",
    "\n",
    "!pip install xgboost\n",
    "\n",
    "\n",
    "#### Instalar versão mais recente do XGBoost a partir do Github\n",
    "\n",
    "git clone --recursive https://github.com/dmlc/xgboost\n",
    "\n",
    "cd xgboost\n",
    "\n",
    "cp make/minimum.mk ./config.mk\n",
    "\n",
    "make -j4\n",
    "\n",
    "cd python-package\n",
    "\n",
    "sudo python setup.py install\n",
    "\n",
    "#### Instalar Windows\n",
    "\n",
    "Download do pacote xgboost em https://www.lfd.uci.edu/~gohlke/pythonlibs/ , escolher de acordo com a versão do python e arquitetura, por exemplo: xgboost‑0.7‑cp36‑cp36m‑win_amd64.whl , Python 3.5 , arquitetura 64bits\n",
    "\n",
    "Executar pip install xgboost-0.7-cp36-cp36m-win_amd64.whl , no cmd\n",
    "\n",
    "Ou\n",
    "\n",
    "Baixar o pacote no diretório do jupyter\n",
    "\n",
    "Executar: !pip install xgboost , pelo jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos modulos\n",
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregando os dados\n",
    "url = \"http://datascienceacademy.com.br/blog/aluno/Python-Spark/Datasets/pima-data.csv\"\n",
    "colunas = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df = read_csv(url, names = colunas)\n",
    "array = df.values\n",
    "\n",
    "# Separando o array em componentes de input e output\n",
    "x = array[:,0:8]\n",
    "y = array[:,8]\n",
    "\n",
    "# Definindo o tamanho dos dados de treino e de teste\n",
    "teste_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "# Criando o dataset de treino e de teste\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, test_size = teste_size, random_state = seed)\n",
    "\n",
    "# Criando o modelo\n",
    "modelo = XGBClassifier()\n",
    "modelo.fit(x_treino, y_treino)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(modelo)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = modelo.predict(x_teste)\n",
    "previsoes = [round(value) for value in y_pred]\n",
    "\n",
    "# Avaliando as previsões\n",
    "accuracy = accuracy_score(y_teste, previsoes)\n",
    "print('Acurácia: %.2f%%' %(accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
